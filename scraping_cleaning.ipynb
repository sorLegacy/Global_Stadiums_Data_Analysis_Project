{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.webdriver.common.by import By\n",
        "from webdriver_manager.chrome import ChromeDriverManager\n",
        "import time\n",
        "import re\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "JnaARsCQElIG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R4E6ywiBD9od"
      },
      "outputs": [],
      "source": [
        "def get_wikipedia_data(url):\n",
        "    s=Service(ChromeDriverManager().install())\n",
        "    driver = webdriver.Chrome(service=s)\n",
        "    driver.maximize_window()\n",
        "    driver.get(url)\n",
        "    time.sleep(5)\n",
        "\n",
        "    # Locate the table and rows\n",
        "    table = driver.find_element(By.XPATH, \"/html/body/div[2]/div/div[3]/main/div[3]/div[3]/div[1]/table[3]\")\n",
        "    rows = table.find_elements(By.XPATH, \"/html/body/div[2]/div/div[3]/main/div[3]/div[3]/div[1]/table[3]/tbody/tr[*]\")\n",
        "\n",
        "    data = []\n",
        "\n",
        "    # Extract data from each row\n",
        "    for i in range(1, len(rows)):\n",
        "        tds = rows[i].find_elements(By.TAG_NAME, \"td\")\n",
        "        if len(tds) >= 7:\n",
        "            values = {\n",
        "                'rank': i,\n",
        "                'stadium': tds[0].text.strip(),\n",
        "                'capacity': tds[1].text.strip().replace(',', '').replace('.', ''),\n",
        "                'region': tds[2].text.strip(),\n",
        "                'country': tds[3].text.strip(),\n",
        "                'city': tds[4].text.strip(),\n",
        "                'images': 'https://' + tds[5].find_element(By.TAG_NAME, 'img').get_attribute('src').split(\"//\")[1] if tds[5].find_elements(By.TAG_NAME, 'img') else 'NO_IMAGE',\n",
        "                'home_team': tds[6].text.strip(),\n",
        "            }\n",
        "            data.append(values)\n",
        "    return data\n",
        "\n",
        "def write_data_to_csv(data):\n",
        "    df = pd.DataFrame(data)\n",
        "    # Remove the number in brackets\n",
        "    df['capacity'] = df['capacity'].str.replace(r'\\[\\d+\\]', '', regex=True)\n",
        "    df['capacity'] = df['capacity'].astype(int)\n",
        "\n",
        "    df.to_csv(r'C:\\Users\\omarn\\Downloads\\stadiums.csv', index=False)\n",
        "    print('Data written to stadiums.csv')\n",
        "\n",
        "# URL of the Wikipedia page to scrape\n",
        "url = \"https://en.wikipedia.org/wiki/List_of_association_football_stadiums_by_capacity\"\n",
        "stadium_data = get_wikipedia_data(url)\n",
        "write_data_to_csv(stadium_data)"
      ]
    }
  ]
}